Problem Scenario 28 : You need to implement near real time solutions for collecting information when submitted in file with below information. 
You have been given below directory location (it not available than create it) /tmp/spooldir2 . 
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume/primary as well as /tmp/flume/secondary location. 
However, note that /tmp/flume/secondary is optional, if transaction failed which writes in this directory need not to be rollback. 
Write a flume configuration file named flume8.conf and use it to load data in hdfs with following additional properties . 

1. Spool /tmp/spooldir2 directory 
2. File prefix in hdfs sholuld be events 
3. File suffix should be .log 
4. If file is not commited and in use than it should have _ as prefix.
5. Data should be written as text to hdfs 
===================================================================
Solution : 

Step 1 : Create directory 
mkdir /tmp/spooldir2 
step 2 : Create flume configuration file, with below configuration for source, sink and channel and save it in flume8.conf. 
agent1 .sources = source1 
agent1.sinks = sink1a sink1b 
agent1 .channels = channel1a channel1b 

agent1 .sources.source1 .channels = channel1a channel1b 
agent1 .sources.source1 .selector.type = replicating 
agent1. sources. Source1. selector. optional = channel1b 
agent1 .sinks.sink1a.channel = channel1a 
agent1 .sinks.sink1b.channel = channel1b 
agent1 .sources.source1 .type = spooldir 
agent1 .sources.source1 .spoolDir = /tmp/spooldir2 

agent .sinks.sink1a.type = hdfs 
agent1 .sinks.sink1a.hdfs.path = /tmp/flume/primary 
agent1 .sinks.sink1a.hdfs.fileprefix = events 
agent1 .sinks.sink1a.hdfs.fileSuffix = .log 
agent1 .sinks.sink1a.hdfs.fileType = DataStream
 
agent1 .sinks.sink1a.type = hdfs 
agent1 .sinks.sink1a.hdfs.path = /tmp/flume/primary 
agent1 .sinks.sink1a.hdfs.filepretix = events 
agent1 .sinks.sink1a.hdfs.fileSuffix = .log 
agent1 .sinks.sink1a.hdfs.fileType = DataStream 

agent1 .sinks.sink1b.type = hdfs 
agent1 .sinks.sink1b.hdfs.path = /tmp/flume/secondary 
agent1 .sinks.sink1b.hdfs.fileprefix = events 
agent1 .sinks.sink1b.hdfs.fileSuffix = .log 
agent1.sinks.sink1b.hdfs.fileType = DataStream 

agent1 .channels.channel1a.type = file 
agent1 .channels.channel1b.type = memory 

Step 4 : Run below command which will use this configuration file and append data in hdfs. 
Start flume service : 

flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumecont/flume8.conf --name agent1 
Step 5 : Open another terminal and create a file in /tmp/spooldir2/ 
echo “IBM,100,20160104”>> /tmp/spooldir2/.bb.txt 
echo”IBM,103,20160105” >> /tmp/spooldir2/.bb.txt 
mv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt 

After few mins 
echo “IBM,100.2,20160104” >> /tmp/spooldir2/.dr.txt 
echo “IBM,103.1,20160105” >> /tmp/spooldir2/.dr.txt 
mv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt 
